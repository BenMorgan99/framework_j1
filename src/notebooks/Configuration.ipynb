{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d88f50-a37c-4e68-bdc5-eec06f304ab1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Requirements"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType, DoubleType, DateType, DecimalType, BooleanType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321295ce-18ab-4378-867b-dbf30857fcfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "KeyVault"
    }
   },
   "outputs": [],
   "source": [
    "# Set KeyVault Variables:\n",
    "secret_scope         = \"dbscope\"\n",
    "storage_account_name = dbutils.secrets.get(scope=secret_scope, key=\"storageAccountName\")\n",
    "sas_token            = dbutils.secrets.get(scope=secret_scope, key=\"sastoken\")\n",
    "container_name       = dbutils.secrets.get(scope=secret_scope, key=\"containerName\")\n",
    "\n",
    "# SQL KeyVault Variables:\n",
    "server_name          = dbutils.secrets.get(scope=secret_scope, key=\"serverName\")\n",
    "metadata_db          = dbutils.secrets.get(scope=secret_scope, key=\"metadataDatabaseName\")\n",
    "totesys_db           = dbutils.secrets.get(scope=secret_scope, key=\"totesysDatabaseName\")\n",
    "db_user              = dbutils.secrets.get(scope=secret_scope, key=\"sqlUser\")\n",
    "db_password          = dbutils.secrets.get(scope=secret_scope, key=\"sqlPassword\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6437c01-11bf-4235-9352-30ecc1a95604",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark configuration:\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\") \n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\") \n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\", sas_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8cf051-e2ed-4a25-86d3-7cc71c728637",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FilePaths"
    }
   },
   "outputs": [],
   "source": [
    "# Set locations to dataLake:\n",
    "BRONZE          = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/framework-j1/BRONZE\"\n",
    "SILVER          = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/framework-j1/SILVER\"\n",
    "GOLD            = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/framework-j1/GOLD\"\n",
    "\n",
    "date_path       = datetime.datetime.now().strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0dfdbd-7be2-4bc8-92f8-3aa4c58e1da9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Type Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "type_dict = {'int': IntegerType(), 'string': StringType(), 'decimal': DecimalType(), 'boolean': BooleanType(), 'datetime': DateType(), 'date': DateType(), 'timestamp': TimestampType(), 'decimal(10,2)': DecimalType(10,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a180c279-b3f0-49e5-9968-1637d5f5ba00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Query framework-j1-db (metadata) database to collect entity names:\n",
    "def query_entityNames(query):\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", server_name)\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", db_user)\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", metadata_db)\n",
    "    .option(\"query\", query)\n",
    "    .load()\n",
    "  ).collect()[0]['entityNames']\n",
    "\n",
    "# Query framework-j1-db (metadata) database:\n",
    "def query_sourceEntity(query):\n",
    "# Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", server_name)\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", db_user)\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", metadata_db)\n",
    "    .option(\"query\", query)\n",
    "    .load()\n",
    "  ).collect()\n",
    "\n",
    "# Query the totesys database and return a dataframe:\n",
    "def query_totesys(query):\n",
    "  return (spark.read\n",
    "  .format(\"sqlserver\")\n",
    "  .option(\"host\", server_name)\n",
    "  .option(\"port\", \"1433\")\n",
    "  .option(\"user\", db_user)\n",
    "  .option(\"password\", db_password)\n",
    "  .option(\"database\", totesys_db)\n",
    "  .option(\"query\", query)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# Function to return the bronze location for the given entity:\n",
    "def entity_bronze(e):\n",
    "  q = f\"\"\"\n",
    "  SELECT bronzeLocation FROM sourceEntity\n",
    "  WHERE entityName = '{e}'\n",
    "  \"\"\"\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", server_name)\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", db_user)\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", metadata_db)\n",
    "    .option(\"query\", q)\n",
    "    .load()\n",
    "  ).collect()[0]['bronzeLocation']\n",
    "\n",
    "# Function to return the silver location for the given entity:\n",
    "def entity_silver(e):\n",
    "  q = f\"\"\"\n",
    "  SELECT silverLocation FROM sourceEntity\n",
    "  WHERE entityName = '{e}'\n",
    "  \"\"\"\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", server_name)\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", db_user)\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", metadata_db)\n",
    "    .option(\"query\", q)\n",
    "    .load()\n",
    "  ).collect()[0]['silverLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14b2de0-c03d-46c6-be93-117bc2b5af61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation Functions"
    }
   },
   "outputs": [],
   "source": [
    "def drop_unwanted_columns_transformation(df):\n",
    "    \"\"\"\n",
    "    This function accepts a dataframe as an argument. \n",
    "    It reads from the global scope; 'entityIngestionColumns'\n",
    "    It filters entityIngestionColumns based on whether or not\n",
    "    the required key has the value False. \n",
    "    It collects the columns with the false value and drops them.columns\n",
    "    The function returns a dataframe.\n",
    "    \"\"\"\n",
    "    # Read from 'entityColumns' to establish unwanted column names:\n",
    "    drop_cols = [x['columnName'] for x in columns if not x['required']]\n",
    "\n",
    "    # Iterate thorugh unwanted columns and drop them:\n",
    "    for col in drop_cols:\n",
    "        df = df.drop(col)\n",
    "\n",
    "    return df\n",
    "\n",
    "def cast_data_types_transformation(df):\n",
    "    \"\"\"\n",
    "    Args: dataframe\n",
    "    Returns: dataframe\n",
    "\n",
    "    This function accepts a dataframe as an argument. \n",
    "    It reads from the global scope; 'entityIngestionColumns' and\n",
    "    casts data types based on the dataType key.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine columns in dataframe:\n",
    "    df_cols = df.columns\n",
    "\n",
    "    # Cast correct datatype:\n",
    "    for col in columns:\n",
    "        # Check column exists in df_cols:\n",
    "        if col['columnName'] in df_cols:\n",
    "            # Cast datatype:\n",
    "            df = df.withColumn(col['columnName'], df[col['columnName']].cast(type_dict[col['dataType']]))\n",
    "    return df\n",
    "\n",
    "def drop_duplicates_transformation(df):\n",
    "    \"\"\"\n",
    "    Arg: dataframe\n",
    "    Returns: dataframe\n",
    "\n",
    "    This function takes a dataframe and seeks\n",
    "    to drop duplicate values.\n",
    "    \"\"\"\n",
    "    # Determine current dataframe columns:\n",
    "    df_columns = df.columns\n",
    "    print('Dataframe columns; ', df_columns)\n",
    "    \n",
    "    # Iterate through columns and check not primary_key:\n",
    "    duplicate_columns = [x['columnName'] for x in columns if x['columnName'] in df_columns and not x['primary_key']]\n",
    "    \n",
    "    df = df.dropDuplicates(duplicate_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5923dd0-02bf-4f32-89d9-2b1ab0cddd75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through an object to create a schema:\n",
    "def create_struct(obj):\n",
    "    # Map strings to dataTypes:\n",
    "    type_dict = {'int': IntegerType(), 'string': StringType(), 'decimal': DecimalType(), 'boolean': BooleanType(), 'date': DateType(), 'timestamp': TimestampType()}\n",
    "    \n",
    "    # Create Schema\n",
    "    struct = StructType([\n",
    "        StructField(value['columnName'], type_dict[value['dataType']], True) \n",
    "        for value in obj\n",
    "    ])\n",
    "    return struct"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Configuration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
