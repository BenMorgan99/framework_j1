{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d88f50-a37c-4e68-bdc5-eec06f304ab1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Requirements"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType, DoubleType, DateType, DecimalType, BooleanType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321295ce-18ab-4378-867b-dbf30857fcfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "KeyVault"
    }
   },
   "outputs": [],
   "source": [
    "# Set KeyVault Variables:\n",
    "secret_scope         = \"carbonScope\"\n",
    "storage_account_name = dbutils.secrets.get(scope=secret_scope, key=\"storageAccountName\")\n",
    "sas_token            = dbutils.secrets.get(scope=secret_scope, key=\"sasToken\")\n",
    "db_password          = dbutils.secrets.get(scope=secret_scope, key=\"connectionStringTotesys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6437c01-11bf-4235-9352-30ecc1a95604",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark configuration:\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SAS\") \n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\") \n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account_name}.dfs.core.windows.net\", sas_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8cf051-e2ed-4a25-86d3-7cc71c728637",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "FilePaths"
    }
   },
   "outputs": [],
   "source": [
    "# Set locations to dataLake:\n",
    "RAW             = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/RAW\"\n",
    "TRANSFORM       = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/TRANSFORM\"\n",
    "CURATE          = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/CURATE\"\n",
    "\n",
    "BRONZE          = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/BRONZE\"\n",
    "SILVER          = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/SILVER\"\n",
    "GOLD            = \"abfss://carbonone@sjpcarbon.dfs.core.windows.net/framework-j1/GOLD\"\n",
    "\n",
    "date_path       = datetime.datetime.now().strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a180c279-b3f0-49e5-9968-1637d5f5ba00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through an object to create a schema:\n",
    "def create_struct(obj):\n",
    "    # Map strings to dataTypes:\n",
    "    type_dict = {'int': IntegerType(), 'string': StringType(), 'decimal': DecimalType(), 'boolean': BooleanType(), 'date': DateType(), 'timestamp': TimestampType()}\n",
    "    \n",
    "    # Create Schema\n",
    "    schema = StructType([\n",
    "        StructField(value['columnName'], type_dict[value['dataType']], True) \n",
    "        for value in obj\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "# Query framework-j1-db (metadata) database and return dataframe:\n",
    "def query_entityNames(query):\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", \"framework-j1-sv.database.windows.net\")\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", \"adminsjp\")\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", \"framework-j1-db\")\n",
    "    .option(\"query\", query)\n",
    "    .load()\n",
    "  ).collect()[0]['entityNames']\n",
    "\n",
    "# Query the totesys database and return a dataframe:\n",
    "def query_totesys(query):\n",
    "  return (spark.read\n",
    "  .format(\"sqlserver\")\n",
    "  .option(\"host\", \"framework-j1-sv.database.windows.net\")\n",
    "  .option(\"port\", \"1433\")\n",
    "  .option(\"user\", \"adminsjp\")\n",
    "  .option(\"password\", db_password)\n",
    "  .option(\"database\", \"totesys\")\n",
    "  .option(\"query\", query)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "# Function to return the bronze location for the given entity:\n",
    "def entity_bronze(e):\n",
    "  q = f\"\"\"\n",
    "  SELECT bronzeLocation FROM sourceEntity\n",
    "  WHERE entityName = '{e}'\n",
    "  \"\"\"\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", \"framework-j1-sv.database.windows.net\")\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", \"adminsjp\")\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", \"framework-j1-db\")\n",
    "    .option(\"query\", q)\n",
    "    .load()\n",
    "  ).collect()[0]['bronzeLocation']\n",
    "\n",
    "# Function to return the silver location for the given entity:\n",
    "def entity_silver(e):\n",
    "  q = f\"\"\"\n",
    "  SELECT silverLocation FROM sourceEntity\n",
    "  WHERE entityName = '{e}'\n",
    "  \"\"\"\n",
    "  # Create and return dataFrame:\n",
    "  return (spark.read\n",
    "    .format(\"sqlserver\")\n",
    "    .option(\"host\", \"framework-j1-sv.database.windows.net\")\n",
    "    .option(\"port\", \"1433\")\n",
    "    .option(\"user\", \"adminsjp\")\n",
    "    .option(\"password\", db_password)\n",
    "    .option(\"database\", \"framework-j1-db\")\n",
    "    .option(\"query\", q)\n",
    "    .load()\n",
    "  ).collect()[0]['silverLocation']"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Configuration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
